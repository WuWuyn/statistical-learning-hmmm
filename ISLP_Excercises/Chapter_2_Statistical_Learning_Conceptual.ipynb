{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b50de8-7555-4f18-9606-7fce758fc6c3",
   "metadata": {},
   "source": [
    "# Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75083d70-8d09-4ae7-8a6b-54002aad484a",
   "metadata": {},
   "source": [
    "## Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8aa61c-64d2-4edd-8640-1f6a333e111c",
   "metadata": {},
   "source": [
    "**Review Core Concepts:**\n",
    "- **Flexibility:** Refers to how complex a model the statiscal learning method can fit. High flexibility allows fitting intricate patterns (e.g., non-linear relationshop), while low flexibility imposes stronger constraints (e.g., linearity).\n",
    "- **Inflexible Methods:** They have **low variance** (the fitted model doesn't change much with different training sets) but potentially **high bias** (the model's assumptions might be too simple to capture the true underlying relationship f). (Example: Linear Regression...)\n",
    "- **Flexible Methods:** They have **low bias** (can approximate complex true relationships f) but potentially **high variance** (can fit the noise in the training data, leading to overfitting and poor perfomance on unseen data). (Example: KNN with small K, high-degree polynomial regression...)\n",
    "- **Perfomance:** Typically measured by prediction accuracy on unseen **test data**. This is related to the Test Mean Squared Error (MSE) for regression or Test Error Rate for classification.\n",
    "- **Bias-Variance Trade-off:** TestError $\\approx$ Bias^2 + Variance + Irreducible Error($\\sigma^2$). The goal is to minimize this total error. Increasing flexibility generally decreases bias but increases variance. The optimal method balances this trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad46110-6127-433c-b8cf-46404470789b",
   "metadata": {},
   "source": [
    "**Solved:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be600f44-ba38-4e68-b45c-5fb444812b56",
   "metadata": {},
   "source": [
    "**(a) The sample size $\\textit{n}$ is extremely large, and the number of predictors $\\textit{p}$ is small.**\n",
    "- Expected Performance: A flexible method will likely perform better.\n",
    "- Justification:\n",
    "    - Bias: A flexible method has lower bias and can capture potentially complex underlying relationships between the predictors and the response.\n",
    "    - Variance: The main drawback of flexible methods is high variance (overfitting). However, with an *extremely large sample size (**n**)*, the risk of overfitting decreases significantly. The model can learn complex patterns from the abundant data without being overly influenced by the noise in any small subset of it.\n",
    "    - Trade-off: Since large **n** mitigates the high variance issue associated with flexible models, we can benefit from their low bias to achieve better overall performance, especially if the true relationship **f** has some non-linearity. The small number of predictors **p** also helps, avoiding the \"curse of dimensionality\" which can make even large datasets seem sparse and hamper flexible methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f036954-19a5-4428-bf33-a293fe9bcf49",
   "metadata": {},
   "source": [
    "**(b) The number of predictors *p* is extremely large, and the number of observations *n* is small.**\n",
    "- Expected Performance: An inflexible method will likely perform better.\n",
    "- Justification:\n",
    "    - Bias: An inflexible method might have higher bias if the true relationship f is complex\n",
    "    - Variance: When p >> n (many predictors, few observations), flexible methods have too much freedom. They can find spurious patterns in the training data and fit the noise almost perfectly, leading to extreme overfitting and very high variance. Their performance on unseen test data will be very poor\n",
    "    - Trade-off: In the p >> n scenario, controlling variance is paramount. Inflexible methods, by imposing strong structural assumptions (like linearity), significantly restrict the model complexity and thus keep the variance low. Even if they introduce some bias, the massive reduction in variance compared to a flexible model typically leads to better test performance. Flexible methods suffer greatly from the curse of dimensionality here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8b283-7094-4c5e-99e8-94eefd0f8b0f",
   "metadata": {},
   "source": [
    "**(c) The relationship between the predictors and response is highly non-linear**\n",
    "- Expected Perfomance: A flexible method will likely perform better\n",
    "- Jusitfication:\n",
    "    - Bias: Inflexible methods (like linear regression) inherently assume a simple, often linear, structure. They will be unable to capture the tru non-linear patterns, resulting in high bias. Flexible methods are specifically designed to handle such complex, non-linear structures, and thus will have much lower bias\n",
    "    - Variance: Flexible methods will still generally have higher variance than inflexible ones.\n",
    "    - Trade-off: Because the true relationship is highly non-linear, the high bias of an inflexible method will be a major source of error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67629b4-4fdc-4146-988f-87943265469c",
   "metadata": {},
   "source": [
    "**(d) The variance of the error terms, i.e. $\\sigma^2=Var(\\epsilon)$, is extremely high**\n",
    "- Expected Performance: An inflexible method will likely perform better\n",
    "- Justification:\n",
    "    - Bias: The bias characteristics remain as usual (flexible = low bias, inflexible = potentially high bias if f is complex)\n",
    "    - Variance: Due to the extremely high variance of error terms, there is a lot of noise in that data. Flexible methods, due to their ability to fit complex patterns, are highly susceptible to fitting this noise in the training data. This leads to a model that wiggles a lot to accommodate noisy points, resulting in high variance (overfitting). Inflexible methods are less sensitive to individual noisy data points\n",
    "    - Trade-off: When $\\sigma^2$ is very high, the \"signal\" (f) is harder to discern from the \"noise\" ($\\epsilon$). Trying to capture intricate patterns with a flexible model is likely just fitting the noise, dramatically increasing the model's variance. An inflexible method, while potentially missing some nuances of f (higher bias), will avoid chasing the noise and maintain lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d53103-4581-4fad-b849-238ecd62a2d7",
   "metadata": {},
   "source": [
    "## Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b3107-dafd-4cfd-a583-03afac2ca7f9",
   "metadata": {},
   "source": [
    "**Review Core Concepts:**\n",
    "- **Regression vs. Classification:**\n",
    "    - Regression: The response variable (Y) is quantitive (numerical). We want to predict a continuous value (e.g., salary, price, percentage change)\n",
    "    - Classification: The response variable (Y) is qualitative (categorical). We want to predict a class label (e.g., success/failure, spam/ham, industry type)\n",
    "- **Inference vs. Prediction:**\n",
    "    - Inference: The primary goal is to understand the relationship between the predictors (X) and the response (Y). We want to know *which* predictors affect the response and *how*. The exact form and interpretation of the model **f** are important\n",
    "    - Prediction: The primary goal is to accurately predict the value of the response (Y) for new observations, given their predictor value (X). The exact form of the model **f** mgiht be treated as a \"black box\" as long as it yields accurate predictions\n",
    "- **n:** The number of observations in the dataset\n",
    "- **p:** The number of predictor variables (features) available for each observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb9f4b8-47fb-4c9d-8614-c25bcb99d3b7",
   "metadata": {},
   "source": [
    "**Solved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65835f-29e8-4e16-b8f9-359b5ee27ec5",
   "metadata": {},
   "source": [
    "**Question (a)**\n",
    "- Problem Type: Regression. The response variable we are interested in is CEO salary which is a numerical value\n",
    "- Goal: Inference. The stated goal is \"understanding which factors affect CEO salary\"\n",
    "- n = 500 (the top 500 firms)\n",
    "- p = 3 (profit, number of employees, industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd2d25-6442-4c5e-966b-44259ba5dc8a",
   "metadata": {},
   "source": [
    "**Question (b)**\n",
    "- Problem Type: Classification. The reponse variable is whether a product launch is a *success* or a *failure*\n",
    "- Goal: Prediction. The goal is to know whether it will be a success or a failure involving using the model built from pass data to predict the outcome for a future, unsenn product\n",
    "- n = 20 (Data was collected on \"20 similar products\")\n",
    "- p = 13 (price charged, marketing budget, competition price, and ten other variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac45ac0-8594-409a-9974-f53f170d324b",
   "metadata": {},
   "source": [
    "**Question (c)**\n",
    "- Problem Type: Regression. The response variable is the % change in the USD/Euro exchange rate. A percentage change is a numerical value.\n",
    "- Goal: Prediction. The goal is to predict the % change in the USD/Euro exchange rate\n",
    "- n $\\approx$ 52 weeks (weekly data for all of 2012) \n",
    "- p = 3 (the % change in the US market, the % change in the British market, the % change in the German market)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d24136-5342-4611-966a-1e5ce1d82338",
   "metadata": {},
   "source": [
    "## Excercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9e311-48d8-43a5-b6d1-4c270b9cb02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb87e10-4638-418b-ae30-7cbad9c2bba4",
   "metadata": {},
   "source": [
    "## Excercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431a9fe-939e-4bf7-8033-a8d4be986cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63b9190c-0e87-4dcf-8b8f-c1695de9035a",
   "metadata": {},
   "source": [
    "## Excercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e0f64-9a95-4df0-a792-0f58a11f4079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "924039cf-490f-4a16-94de-f928433d7c88",
   "metadata": {},
   "source": [
    "## Excercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01f0c0-9757-4277-a801-15b86e6a135f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5660363b-3e6e-4bd3-9d77-fd7dc9079a61",
   "metadata": {},
   "source": [
    "## Excercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d79c7e-1070-4468-bd20-268d8882b465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
